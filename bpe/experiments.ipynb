{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original text is:\n",
      "-----------------\n",
      "北 京 大 学\n",
      "PEKING UNIVERSITY\n",
      "北 京 ⼤ 学 研 究 ⽣ ⼿ 册\n",
      "（ 2 0 2 3 版 ）\n",
      "北 京 ⼤ 学 研 究 ⽣ 院\n",
      "2 0 2 3 年 8 ⽉\n",
      "习近平对研究生教育工作作\n",
      "-----------------\n",
      "The decoded text is:\n",
      "-----------------\n",
      "北 京 大 学\n",
      "PEKING UNIVERSITY\n",
      "北 京 ⼤ 学 研 究 ⽣ ⼿ 册\n",
      "（ 2 0 2 3 版 ）\n",
      "北 京 ⼤ 学 研 究 ⽣ 院\n",
      "2 0 2 3 年 8 ⽉\n",
      "习近平对研究生教育工作作\n",
      "-----------------\n",
      "The text decoded is the same as the origin.\n"
     ]
    }
   ],
   "source": [
    "from tokenizer import Tokenizer\n",
    "\n",
    "def main():\n",
    "    # Reading the text provided\n",
    "    input_path = \"manual.txt\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        original = f.read()\n",
    "    print(\"The original text is:\")\n",
    "    print('-----------------')\n",
    "    print(original[:100])\n",
    "    print('-----------------')\n",
    "\n",
    "    # Train\n",
    "    tok = Tokenizer()\n",
    "    tok.train(original, vocab_size=1024)\n",
    "\n",
    "    # Encode & Decode\n",
    "    ids = tok.encode(original)\n",
    "    decoded = tok.decode(ids)\n",
    "    print(\"The decoded text is:\")\n",
    "    print('-----------------')\n",
    "    print(decoded[:100])\n",
    "    print('-----------------')\n",
    "\n",
    "    # Check the coherence\n",
    "    if decoded == original:\n",
    "        print(\"The text decoded is the same as the origin.\")\n",
    "    else:\n",
    "        print(\"The text decoded is different from the origin.\")\n",
    "        # To view the detail:\n",
    "        import difflib\n",
    "        diff = difflib.unified_diff(\n",
    "            original.splitlines(True),\n",
    "            decoded.splitlines(True),\n",
    "            fromfile=\"original\",\n",
    "            tofile=\"decoded\",\n",
    "        )\n",
    "        print(\"\".join(diff))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: Originated as the Imperial University of Peking in 1898, Peking University was China’s first national comprehensive university and the supreme education authority at the time. Since the founding of the People’s Republic of China in 1949, it has developed into a comprehensive university with fundamental education and research in both humanities and science. The reform and opening-up of China in 1978 has ushered in a new era for the University unseen in history. And its merger with Beijing Medical University in 2000 has geared itself up for all-round and vibrant growth in such fields as science, engineering, medicine, agriculture, humanities and social sciences. Supported by the “211 Project” and the “985 Project”, the University has made remarkable achievements, such as optimizing disciplines, cultivating talents, recruiting high-caliber teachers, as well as teaching and scientific research, which paves the way for a world-class university.\n",
      "--------------------\n",
      "GPT-2 Token IDs: [11610, 3898, 355, 262, 11773, 2059, 286, 350, 18754, 287, 46244, 11, 350, 18754, 2059, 373, 2807, 447, 247, 82, 717, 2260, 9815, 6403, 290, 262, 17700, 3707, 4934, 379, 262, 640, 13, 4619, 262, 16636, 286, 262, 4380, 447, 247, 82, 2066, 286, 2807, 287, 24977, 11, 340, 468, 4166, 656, 257, 9815, 6403, 351, 7531, 3707, 290, 2267, 287, 1111, 47824, 290, 3783, 13, 383, 4975, 290, 4756, 12, 929, 286, 2807, 287, 15524, 468, 47098, 287, 257, 649, 6980, 329, 262, 2059, 29587, 287, 2106, 13, 843, 663, 24589, 351, 11618, 8366, 2059, 287, 4751, 468, 31394, 2346, 510, 329, 477, 12, 744, 290, 21266, 3349, 287, 884, 7032, 355, 3783, 11, 8705, 11, 9007, 11, 14510, 11, 47824, 290, 1919, 19838, 13, 36848, 416, 262, 564, 250, 21895, 4935, 447, 251, 290, 262, 564, 250, 42250, 4935, 447, 251, 11, 262, 2059, 468, 925, 11004, 16970, 11, 884, 355, 45780, 29861, 11, 45414, 18054, 11, 16517, 1029, 12, 43288, 7799, 11, 355, 880, 355, 7743, 290, 5654, 2267, 11, 543, 279, 3080, 262, 835, 329, 257, 995, 12, 4871, 6403, 13]\n",
      "--------------------\n",
      "BPE Manual Token IDs: [79, 114, 105, 103, 105, 110, 97, 116, 101, 100, 32, 97, 115, 32, 116, 104, 101, 32, 73, 109, 112, 101, 114, 105, 97, 108, 32, 85, 110, 105, 118, 101, 114, 115, 105, 116, 121, 32, 111, 102, 592, 101, 107, 105, 110, 103, 32, 105, 110, 699, 56, 57, 56, 44, 592, 101, 107, 105, 110, 103, 32, 85, 110, 105, 118, 101, 114, 115, 105, 116, 121, 32, 119, 97, 115, 32, 67, 104, 105, 110, 97, 397, 153, 115, 32, 102, 105, 114, 115, 116, 32, 110, 97, 116, 105, 111, 110, 97, 108, 32, 99, 111, 109, 112, 114, 101, 104, 101, 110, 115, 105, 118, 101, 32, 117, 110, 105, 118, 101, 114, 115, 105, 116, 121, 32, 97, 110, 100, 32, 116, 104, 101, 32, 115, 117, 112, 114, 101, 109, 101, 32, 101, 100, 117, 99, 97, 116, 105, 111, 110, 32, 97, 117, 116, 104, 111, 114, 105, 116, 121, 32, 97, 116, 32, 116, 104, 101, 32, 116, 105, 109, 101, 566, 83, 105, 110, 99, 101, 32, 116, 104, 101, 32, 102, 111, 117, 110, 100, 105, 110, 103, 32, 111, 102, 32, 116, 104, 101, 592, 101, 111, 112, 108, 101, 397, 153, 115, 32, 82, 101, 112, 117, 98, 108, 105, 99, 32, 111, 102, 32, 67, 104, 105, 110, 97, 32, 105, 110, 699, 57, 52, 57, 44, 32, 105, 116, 32, 104, 97, 115, 32, 100, 101, 118, 101, 108, 111, 112, 101, 100, 32, 105, 110, 116, 111, 32, 97, 32, 99, 111, 109, 112, 114, 101, 104, 101, 110, 115, 105, 118, 101, 32, 117, 110, 105, 118, 101, 114, 115, 105, 116, 121, 32, 119, 105, 116, 104, 32, 102, 117, 110, 100, 97, 109, 101, 110, 116, 97, 108, 32, 101, 100, 117, 99, 97, 116, 105, 111, 110, 32, 97, 110, 100, 32, 114, 101, 115, 101, 97, 114, 99, 104, 32, 105, 110, 32, 98, 111, 116, 104, 32, 104, 117, 109, 97, 110, 105, 116, 105, 101, 115, 32, 97, 110, 100, 32, 115, 99, 105, 101, 110, 99, 101, 566, 84, 104, 101, 32, 114, 101, 102, 111, 114, 109, 32, 97, 110, 100, 32, 111, 112, 101, 110, 105, 110, 103, 45, 117, 112, 32, 111, 102, 32, 67, 104, 105, 110, 97, 32, 105, 110, 699, 57, 55, 56, 32, 104, 97, 115, 32, 117, 115, 104, 101, 114, 101, 100, 32, 105, 110, 32, 97, 32, 110, 101, 119, 32, 101, 114, 97, 32, 102, 111, 114, 32, 116, 104, 101, 32, 85, 110, 105, 118, 101, 114, 115, 105, 116, 121, 32, 117, 110, 115, 101, 101, 110, 32, 105, 110, 32, 104, 105, 115, 116, 111, 114, 121, 566, 65, 110, 100, 32, 105, 116, 115, 32, 109, 101, 114, 103, 101, 114, 32, 119, 105, 116, 104, 32, 66, 101, 105, 106, 105, 110, 103, 32, 77, 101, 100, 105, 99, 97, 108, 32, 85, 110, 105, 118, 101, 114, 115, 105, 116, 121, 32, 105, 110, 684, 48, 48, 48, 32, 104, 97, 115, 32, 103, 101, 97, 114, 101, 100, 32, 105, 116, 115, 101, 108, 102, 32, 117, 112, 32, 102, 111, 114, 32, 97, 108, 108, 45, 114, 111, 117, 110, 100, 32, 97, 110, 100, 32, 118, 105, 98, 114, 97, 110, 116, 32, 103, 114, 111, 119, 116, 104, 32, 105, 110, 32, 115, 117, 99, 104, 32, 102, 105, 101, 108, 100, 115, 32, 97, 115, 32, 115, 99, 105, 101, 110, 99, 101, 44, 32, 101, 110, 103, 105, 110, 101, 101, 114, 105, 110, 103, 44, 32, 109, 101, 100, 105, 99, 105, 110, 101, 44, 32, 97, 103, 114, 105, 99, 117, 108, 116, 117, 114, 101, 44, 32, 104, 117, 109, 97, 110, 105, 116, 105, 101, 115, 32, 97, 110, 100, 32, 115, 111, 99, 105, 97, 108, 32, 115, 99, 105, 101, 110, 99, 101, 115, 566, 83, 117, 112, 112, 111, 114, 116, 101, 100, 32, 98, 121, 32, 116, 104, 101, 32, 554, 50, 49, 49, 592, 114, 111, 106, 101, 99, 116, 558, 32, 97, 110, 100, 32, 116, 104, 101, 32, 554, 57, 56, 53, 592, 114, 111, 106, 101, 99, 116, 558, 44, 32, 116, 104, 101, 32, 85, 110, 105, 118, 101, 114, 115, 105, 116, 121, 32, 104, 97, 115, 32, 109, 97, 100, 101, 32, 114, 101, 109, 97, 114, 107, 97, 98, 108, 101, 32, 97, 99, 104, 105, 101, 118, 101, 109, 101, 110, 116, 115, 44, 32, 115, 117, 99, 104, 32, 97, 115, 32, 111, 112, 116, 105, 109, 105, 122, 105, 110, 103, 32, 100, 105, 115, 99, 105, 112, 108, 105, 110, 101, 115, 44, 32, 99, 117, 108, 116, 105, 118, 97, 116, 105, 110, 103, 32, 116, 97, 108, 101, 110, 116, 115, 44, 32, 114, 101, 99, 114, 117, 105, 116, 105, 110, 103, 32, 104, 105, 103, 104, 45, 99, 97, 108, 105, 98, 101, 114, 32, 116, 101, 97, 99, 104, 101, 114, 115, 44, 32, 97, 115, 32, 119, 101, 108, 108, 32, 97, 115, 32, 116, 101, 97, 99, 104, 105, 110, 103, 32, 97, 110, 100, 32, 115, 99, 105, 101, 110, 116, 105, 102, 105, 99, 32, 114, 101, 115, 101, 97, 114, 99, 104, 44, 32, 119, 104, 105, 99, 104, 32, 112, 97, 118, 101, 115, 32, 116, 104, 101, 32, 119, 97, 121, 32, 102, 111, 114, 32, 97, 32, 119, 111, 114, 108, 100, 45, 99, 108, 97, 115, 115, 32, 117, 110, 105, 118, 101, 114, 115, 105, 116, 121, 46]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from tokenizer import Tokenizer\n",
    "\n",
    "# The sentence needed to be encoded\n",
    "sentence = \"Originated as the Imperial University of Peking in 1898, Peking University was China’s first national comprehensive university and the supreme education authority at the time. Since the founding of the People’s Republic of China in 1949, it has developed into a comprehensive university with fundamental education and research in both humanities and science. The reform and opening-up of China in 1978 has ushered in a new era for the University unseen in history. And its merger with Beijing Medical University in 2000 has geared itself up for all-round and vibrant growth in such fields as science, engineering, medicine, agriculture, humanities and social sciences. Supported by the “211 Project” and the “985 Project”, the University has made remarkable achievements, such as optimizing disciplines, cultivating talents, recruiting high-caliber teachers, as well as teaching and scientific research, which paves the way for a world-class university.\"\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# loading tokenizer from GPT-2\n",
    "GPT2_tokenizer = GPT2Tokenizer.from_pretrained(\"../gpt2\")\n",
    "\n",
    "GPT2_encoded = GPT2_tokenizer.encode(sentence)\n",
    "print(f\"GPT-2 Token IDs: {GPT2_encoded}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# loading tokenizer from manual tokenizer\n",
    "input_path = \"manual.txt\"\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    original = f.read()\n",
    "\n",
    "# Train manual tokenizer\n",
    "BPE_tokenizer = Tokenizer()\n",
    "BPE_tokenizer.train(original, vocab_size=1024)\n",
    "\n",
    "BPE_encoded = BPE_tokenizer.encode(sentence)\n",
    "print(f\"BPE Manual Token IDs: {BPE_encoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: 博士学位论文应当表明作者具有独立从事科学研究工作的能力，并在科学或专门技术上做出创造性的成果。博士学位论文或摘要，应当在答辩前三个月印送有关单位，并经同行评议。学位授予单位应当聘请两位与论文有关学科的专家评阅论文，其中一位应当是外单位的专家。评阅人应当对论文写详细的学术评语，供论文答辩委员会参考。\n",
      "--------------------\n",
      "GPT-2 Token IDs: [39355, 248, 18803, 27764, 99, 19526, 235, 164, 106, 118, 23877, 229, 41753, 242, 37605, 241, 26193, 101, 23626, 236, 43291, 38519, 17739, 115, 17312, 231, 45379, 105, 44165, 233, 20015, 236, 12859, 233, 163, 100, 239, 27764, 99, 163, 254, 242, 163, 102, 114, 32432, 98, 43291, 21410, 47797, 121, 27950, 249, 171, 120, 234, 33176, 114, 28839, 101, 163, 100, 239, 27764, 99, 22755, 244, 10310, 241, 29785, 101, 162, 232, 222, 17312, 107, 41468, 161, 223, 248, 49035, 118, 26344, 249, 34460, 254, 45250, 100, 21410, 22755, 238, 162, 252, 250, 16764, 39355, 248, 18803, 27764, 99, 19526, 235, 164, 106, 118, 23877, 229, 22755, 244, 162, 239, 246, 17358, 223, 171, 120, 234, 41753, 242, 37605, 241, 28839, 101, 163, 18433, 164, 122, 102, 30298, 235, 49011, 10310, 103, 17312, 230, 39355, 108, 34460, 223, 17312, 231, 17739, 111, 39355, 243, 19526, 235, 171, 120, 234, 33176, 114, 163, 119, 237, 28938, 234, 26193, 234, 46237, 226, 164, 106, 106, 16764, 27764, 99, 19526, 235, 162, 236, 230, 12859, 230, 39355, 243, 19526, 235, 41753, 242, 37605, 241, 164, 223, 246, 46237, 115, 10310, 97, 19526, 235, 10310, 236, 164, 106, 118, 23877, 229, 17312, 231, 17739, 111, 27764, 99, 163, 100, 239, 21410, 10310, 241, 22522, 114, 46237, 226, 165, 11805, 164, 106, 118, 23877, 229, 171, 120, 234, 17739, 114, 40792, 31660, 19526, 235, 41753, 242, 37605, 241, 42468, 13783, 244, 39355, 243, 19526, 235, 21410, 10310, 241, 22522, 114, 16764, 46237, 226, 165, 11805, 21689, 41753, 242, 37605, 241, 43380, 117, 164, 106, 118, 23877, 229, 37863, 247, 46237, 99, 163, 119, 228, 21410, 27764, 99, 17312, 107, 46237, 226, 46237, 255, 171, 120, 234, 160, 122, 249, 164, 106, 118, 23877, 229, 163, 18433, 164, 122, 102, 34650, 242, 37772, 246, 27670, 248, 20998, 224, 32003, 225, 16764]\n",
      "--------------------\n",
      "BPE Manual Token IDs: [454, 508, 520, 710, 619, 341, 453, 670, 351, 231, 139, 172, 722, 979, 550, 909, 294, 419, 267, 1022, 690, 326, 909, 349, 429, 499, 373, 128, 270, 175, 493, 951, 431, 755, 854, 776, 267, 950, 272, 454, 508, 349, 230, 145, 152, 404, 261, 520, 326, 792, 709, 443, 537, 705, 283, 176, 903, 652, 497, 690, 481, 534, 332, 376, 494, 272, 929, 497, 520, 560, 152, 268, 183, 257, 164, 290, 457, 408, 652, 691, 267, 967, 944, 408, 261, 417, 385, 333, 290, 520, 569, 546, 497, 267, 967, 272, 944, 314, 520, 433, 408, 823, 268, 166, 948, 267, 450, 376, 748, 261, 555, 155, 408, 792, 445, 542, 407, 272]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from tokenizer import Tokenizer\n",
    "\n",
    "# The sentence needed to be encoded\n",
    "sentence = \"博士学位论文应当表明作者具有独立从事科学研究工作的能力，并在科学或专门技术上做出创造性的成果。博士学位论文或摘要，应当在答辩前三个月印送有关单位，并经同行评议。学位授予单位应当聘请两位与论文有关学科的专家评阅论文，其中一位应当是外单位的专家。评阅人应当对论文写详细的学术评语，供论文答辩委员会参考。\"\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# loading tokenizer from GPT-2\n",
    "GPT2_tokenizer = GPT2Tokenizer.from_pretrained(\"../gpt2\")\n",
    "\n",
    "GPT2_encoded = GPT2_tokenizer.encode(sentence)\n",
    "print(f\"GPT-2 Token IDs: {GPT2_encoded}\")\n",
    "print(\"--------------------\")\n",
    "\n",
    "# loading tokenizer from manual tokenizer\n",
    "input_path = \"manual.txt\"\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    original = f.read()\n",
    "\n",
    "# Train manual tokenizer\n",
    "BPE_tokenizer = Tokenizer()\n",
    "BPE_tokenizer.train(original, vocab_size=1024)\n",
    "\n",
    "BPE_encoded = BPE_tokenizer.encode(sentence)\n",
    "print(f\"BPE Manual Token IDs: {BPE_encoded}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
