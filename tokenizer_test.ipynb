{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x) for x in \"Hello\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(\"Hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        # token -> id\n",
    "        self.token2id = {}\n",
    "        # id -> token\n",
    "        self.id2token = {}\n",
    "        # merge operations in order [(x, y), ...]\n",
    "        self.merges = []\n",
    "\n",
    "    def get_stats(self, corpus):\n",
    "        \"\"\"\n",
    "        统计当前语料中所有相邻符号对的频次\n",
    "        corpus: list of words, each word is list of symbols, e.g. ['t', 'h', 'e', '</w>']\n",
    "        返回：dict mapping (sym_i, sym_{i+1}) -> count\n",
    "        \"\"\"\n",
    "        pairs = {}\n",
    "        for word in corpus:\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i + 1])\n",
    "                pairs[pair] = pairs.get(pair, 0) + 1\n",
    "        return pairs\n",
    "\n",
    "    def merge_pair(self, pair, corpus):\n",
    "        \"\"\"\n",
    "        在所有词上将指定的符号对子合并为单一符号\n",
    "        \"\"\"\n",
    "        merged_token = ''.join(pair)\n",
    "        new_corpus = []\n",
    "        for word in corpus:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                # 如果当前位置及下一位置是我们要合并的 pair，就替换为 merged_token\n",
    "                if i < len(word) - 1 and word[i] == pair[0] and word[i + 1] == pair[1]:\n",
    "                    new_word.append(merged_token)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_corpus.append(new_word)\n",
    "        return new_corpus\n",
    "\n",
    "    def train(self, text, vocab_size):\n",
    "        \"\"\"\n",
    "        训练 BPE 分词器\n",
    "        text: 原始字符串\n",
    "        vocab_size: 希望得到的词表大小（包含所有字符与子词）\n",
    "        \"\"\"\n",
    "        # 1. 构建初始语料：按空格切词，每词末尾加 </w>，并拆成字符列表\n",
    "        words = text.strip().split()\n",
    "        corpus = [list(word) + ['</w>'] for word in words]\n",
    "\n",
    "        # 2. 初始化 vocab 为所有单字符 + </w>\n",
    "        vocab = set()\n",
    "        for w in corpus:\n",
    "            vocab.update(w)\n",
    "\n",
    "        # 3. 迭代合并\n",
    "        while len(vocab) < vocab_size:\n",
    "            pairs = self.get_stats(corpus)\n",
    "            if not pairs:\n",
    "                break\n",
    "            # 选出现频次最高的一对\n",
    "            best_pair = max(pairs, key=pairs.get)\n",
    "            self.merges.append(best_pair)\n",
    "            # 合并语料中的该对子\n",
    "            corpus = self.merge_pair(best_pair, corpus)\n",
    "            # 添加新符号到 vocab\n",
    "            merged_token = ''.join(best_pair)\n",
    "            vocab.add(merged_token)\n",
    "\n",
    "        # 4. 构建 token2id 与 id2token 映射\n",
    "        for idx, token in enumerate(sorted(vocab)):\n",
    "            self.token2id[token] = idx\n",
    "            self.id2token[idx] = token\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        将输入字符串编码为 ID 列表\n",
    "        \"\"\"\n",
    "        words = text.strip().split()\n",
    "        output_ids = []\n",
    "        for word in words:\n",
    "            symbols = list(word) + ['</w>']\n",
    "            # 按训练时 merges 的顺序依次尝试合并\n",
    "            for pair in self.merges:\n",
    "                i = 0\n",
    "                while i < len(symbols) - 1:\n",
    "                    if symbols[i] == pair[0] and symbols[i+1] == pair[1]:\n",
    "                        symbols[i:i+2] = [''.join(pair)]\n",
    "                    else:\n",
    "                        i += 1\n",
    "            # 将每个子词映射到 ID\n",
    "            for sym in symbols:\n",
    "                output_ids.append(self.token2id[sym])\n",
    "        return output_ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        将 ID 列表解码回字符串\n",
    "        \"\"\"\n",
    "        tokens = [self.id2token[i] for i in ids]\n",
    "        words = []\n",
    "        current = []\n",
    "        for t in tokens:\n",
    "            if t == '</w>':\n",
    "                # 遇到词结束符则将 current 拼成一个词\n",
    "                words.append(''.join(current))\n",
    "                current = []\n",
    "            else:\n",
    "                current.append(t)\n",
    "        # 如果最后没有以 </w> 结束，补上剩余\n",
    "        if current:\n",
    "            words.append(''.join(current))\n",
    "        return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [5, 11]\n",
      "Decoded text: low</w>lowest</w>\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    text = \"low lower lowest low\"\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    tokenizer.train(text, vocab_size=20)\n",
    "\n",
    "    inds = tokenizer.encode(\"low lowest\")\n",
    "    print(\"Encoded IDs:\", inds)\n",
    "\n",
    "    s = tokenizer.decode(inds)\n",
    "    print(\"Decoded text:\", s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [10, 16]\n",
      "Decoded text: low</w>lowest</w>\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        # BPE 合并规则：{ (p0, p1): new_id, ... }\n",
    "        self.merges = {}\n",
    "        # vocab 映射：token -> id\n",
    "        self.token2id = {}\n",
    "        self.id2token = {}\n",
    "\n",
    "    def get_stats(self, corpus):\n",
    "        \"\"\"\n",
    "        统计当前语料（list of list of symbols）中所有相邻符号对的出现次数\n",
    "        返回 dict: { (sym_i, sym_{i+1}): count, ... }\n",
    "        \"\"\"\n",
    "        stats = {}\n",
    "        for word in corpus:\n",
    "            for i in range(len(word) - 1):\n",
    "                pair = (word[i], word[i+1])\n",
    "                stats[pair] = stats.get(pair, 0) + 1\n",
    "        return stats\n",
    "\n",
    "    def merge_pair(self, pair, corpus):\n",
    "        \"\"\"\n",
    "        在 corpus 中将所有相邻的 pair=('A','B') 合并为 ['AB']\n",
    "        返回新的 corpus\n",
    "        \"\"\"\n",
    "        merged = pair[0] + pair[1]\n",
    "        new_corpus = []\n",
    "        for word in corpus:\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                # 如果当前位置和下一个是我们要合并的 pair，则替换为 merged\n",
    "                if i < len(word)-1 and word[i] == pair[0] and word[i+1] == pair[1]:\n",
    "                    new_word.append(merged)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_corpus.append(new_word)\n",
    "        return new_corpus\n",
    "\n",
    "    def train(self, text, vocab_size):\n",
    "        \"\"\"\n",
    "        使用 BPE 训练合并规则，生成从 token->id 的映射\n",
    "        \"\"\"\n",
    "        # 1. 构建初始语料：按空格切词，每词末尾加 '</w>' 表示词边界\n",
    "        words = text.strip().split()\n",
    "        corpus = [list(w) + ['</w>'] for w in words]\n",
    "\n",
    "        # 2. 初始 vocab：所有单字符 + '</w>'\n",
    "        vocab = set()\n",
    "        for w in corpus:\n",
    "            vocab.update(w)\n",
    "\n",
    "        # 3. 迭代合并，直到 vocab 大小达到 vocab_size\n",
    "        next_id = 0\n",
    "        while len(vocab) < vocab_size:\n",
    "            stats = self.get_stats(corpus)\n",
    "            if not stats:\n",
    "                break\n",
    "            # 选出现次数最多的 pair\n",
    "            best_pair = max(stats, key=stats.get)\n",
    "            # 给它分配一个新 id\n",
    "            self.merges[best_pair] = vocab_size + len(self.merges)\n",
    "            # 执行合并\n",
    "            corpus = self.merge_pair(best_pair, corpus)\n",
    "            # 更新 vocab\n",
    "            merged_token = best_pair[0] + best_pair[1]\n",
    "            vocab.add(merged_token)\n",
    "\n",
    "        # 4. 构建最终的 token2id/id2token（先把单字符 sorted，再加上所有 merges 生成的 tokens）\n",
    "        all_tokens = sorted([t for t in vocab if len(t)==1 or t == '</w>'])\n",
    "        # merges 生成的新 token\n",
    "        for (p0, p1), idx in sorted(self.merges.items(), key=lambda x: x[1]):\n",
    "            all_tokens.append(p0+p1)\n",
    "\n",
    "        # 映射编号\n",
    "        for i, tok in enumerate(all_tokens):\n",
    "            self.token2id[tok] = i\n",
    "            self.id2token[i] = tok\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        将字符串编码为 token id 列表\n",
    "        \"\"\"\n",
    "        words = text.strip().split()\n",
    "        output_ids = []\n",
    "        for w in words:\n",
    "            symbols = list(w) + ['</w>']\n",
    "            # 按训练时 merges 的顺序依次尝试合并\n",
    "            # merges 按值排序即合并顺序\n",
    "            for (p0, p1), _ in sorted(self.merges.items(), key=lambda x: x[1]):\n",
    "                i = 0\n",
    "                while i < len(symbols)-1:\n",
    "                    if symbols[i] == p0 and symbols[i+1] == p1:\n",
    "                        symbols[i:i+2] = [p0+p1]\n",
    "                    else:\n",
    "                        i += 1\n",
    "            # 转成 id\n",
    "            for s in symbols:\n",
    "                output_ids.append(self.token2id[s])\n",
    "        return output_ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        将 token id 列表解码回字符串\n",
    "        \"\"\"\n",
    "        tokens = [self.id2token[i] for i in ids]\n",
    "        words = []\n",
    "        cur = []\n",
    "        for t in tokens:\n",
    "            if t == '</w>':\n",
    "                words.append(''.join(cur))\n",
    "                cur = []\n",
    "            else:\n",
    "                cur.append(t)\n",
    "        # 如果最后没有以 </w> 结尾，补充\n",
    "        if cur:\n",
    "            words.append(''.join(cur))\n",
    "        return ' '.join(words)\n",
    "\n",
    "\n",
    "# ==== 使用示例 ====\n",
    "if __name__ == \"__main__\":\n",
    "    sample = \"low lower lowest low\"\n",
    "    tok = Tokenizer()\n",
    "    tok.train(sample, vocab_size=20)\n",
    "\n",
    "    seq = tok.encode(\"low lowest\")\n",
    "    print(\"Encoded IDs:\", seq)\n",
    "\n",
    "    txt = tok.decode(seq)\n",
    "    print(\"Decoded text:\", txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded IDs: [108, 111, 119, 32, 108, 111, 119, 101, 115, 116]\n",
      "Decoded text: low lowest\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        # BPE 合并：{ (b1, b2): new_id, ... }\n",
    "        self.merges = {}\n",
    "        # token(id) -> byte sequence\n",
    "        self.id2bytes = {}\n",
    "        # byte sequence -> id\n",
    "        self.bytes2id = {}\n",
    "\n",
    "    def get_stats(self, seq):\n",
    "        \"\"\"统计一个字节序列中所有相邻对的次数\"\"\"\n",
    "        stats = {}\n",
    "        for i in range(len(seq) - 1):\n",
    "            pair = (seq[i], seq[i+1])\n",
    "            stats[pair] = stats.get(pair, 0) + 1\n",
    "        return stats\n",
    "\n",
    "    def merge_pair(self, pair, seq):\n",
    "        \"\"\"在整个序列里把 pair=('x','y') 全部替换成 merged\"\"\"\n",
    "        merged = pair[0] + pair[1]\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i < len(seq)-1 and seq[i]==pair[0] and seq[i+1]==pair[1]:\n",
    "                out.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "        return out\n",
    "\n",
    "    def train(self, text, vocab_size):\n",
    "        # 1) 把全文当作一个“词”，初始序列是 UTF-8 bytes 的 list\n",
    "        data = list(text.encode(\"utf-8\"))\n",
    "        # 2) 初始 vocab 是所有单字节\n",
    "        vocab = set(data)\n",
    "        # 3) 迭代合并最常见的 pair\n",
    "        next_id = 256\n",
    "        seq = [bytes([b]) for b in data]  # 把每个 int->bytes([int])\n",
    "        # id2bytes / bytes2id 先填单字节\n",
    "        for b in range(256):\n",
    "            bt = bytes([b])\n",
    "            self.id2bytes[b] = bt\n",
    "            self.bytes2id[bt] = b\n",
    "\n",
    "        while len(self.id2bytes) < vocab_size:\n",
    "            stats = self.get_stats(seq)\n",
    "            if not stats:\n",
    "                break\n",
    "            # 选出现最多的 pair\n",
    "            best = max(stats, key=stats.get)\n",
    "            merged = best[0] + best[1]\n",
    "            # 加到 merges，并给它分配新的 ID\n",
    "            self.merges[best] = next_id\n",
    "            self.id2bytes[next_id] = merged\n",
    "            self.bytes2id[merged] = next_id\n",
    "            next_id += 1\n",
    "            # 合并序列\n",
    "            seq = self.merge_pair(best, seq)\n",
    "\n",
    "    def encode(self, text):\n",
    "        # 把 text->bytes list，再按 merges 顺序合并\n",
    "        seq = [bytes([b]) for b in text.encode(\"utf-8\")]\n",
    "        for pair, idx in sorted(self.merges.items(), key=lambda x: x[1]):\n",
    "            i = 0\n",
    "            while i < len(seq)-1:\n",
    "                if seq[i]==pair[0] and seq[i+1]==pair[1]:\n",
    "                    seq[i:i+2] = [self.id2bytes[idx]]\n",
    "                else:\n",
    "                    i += 1\n",
    "        # 最后把 bytes token 转成 ID\n",
    "        return [ self.bytes2id[b] for b in seq ]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        # 把 ID 列表还原到 bytes，再一次性 decode\n",
    "        bseq = b\"\".join(self.id2bytes[i] for i in ids)\n",
    "        return bseq.decode(\"utf-8\", errors=\"strict\")\n",
    "\n",
    "\n",
    "# ==== 使用示例 ====\n",
    "if __name__ == \"__main__\":\n",
    "    sample = \"low lower lowest low\"\n",
    "    tok = Tokenizer()\n",
    "    tok.train(sample, vocab_size=20)\n",
    "\n",
    "    seq = tok.encode(\"low lowest\")\n",
    "    print(\"Encoded IDs:\", seq)\n",
    "\n",
    "    txt = tok.decode(seq)\n",
    "    print(\"Decoded text:\", txt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 解码后文本与原文完全一致！\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.merges = {}\n",
    "        self.id2bytes = {}\n",
    "        self.bytes2id = {}\n",
    "\n",
    "    def _get_stats(self, seq):\n",
    "        stats = {}\n",
    "        for i in range(len(seq) - 1):\n",
    "            pair = (seq[i], seq[i+1])\n",
    "            stats[pair] = stats.get(pair, 0) + 1\n",
    "        return stats\n",
    "\n",
    "    def _merge_pair(self, pair, seq):\n",
    "        merged = pair[0] + pair[1]\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i < len(seq) - 1 and seq[i] == pair[0] and seq[i+1] == pair[1]:\n",
    "                out.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "        return out\n",
    "\n",
    "    def train(self, text, vocab_size):\n",
    "        data = list(text.encode(\"utf-8\"))\n",
    "        seq = [bytes([b]) for b in data]\n",
    "\n",
    "        for b in range(256):\n",
    "            bt = bytes([b])\n",
    "            self.id2bytes[b] = bt\n",
    "            self.bytes2id[bt] = b\n",
    "        next_id = 256\n",
    "\n",
    "        while len(self.id2bytes) < vocab_size:\n",
    "            stats = self._get_stats(seq)\n",
    "            if not stats:\n",
    "                break\n",
    "\n",
    "            best = max(stats, key=stats.get)\n",
    "            merged = best[0] + best[1]\n",
    "\n",
    "            self.merges[best] = next_id\n",
    "            self.id2bytes[next_id] = merged\n",
    "            self.bytes2id[merged] = next_id\n",
    "            next_id += 1\n",
    "\n",
    "            seq = self._merge_pair(best, seq)\n",
    "\n",
    "    def encode(self, text):\n",
    "        seq = [bytes([b]) for b in text.encode(\"utf-8\")]\n",
    "\n",
    "        for pair, idx in sorted(self.merges.items(), key=lambda x: x[1]):\n",
    "            i = 0\n",
    "            while i < len(seq) - 1:\n",
    "                if seq[i] == pair[0] and seq[i+1] == pair[1]:\n",
    "                    seq[i:i+2] = [self.id2bytes[idx]]\n",
    "                else:\n",
    "                    i += 1\n",
    "\n",
    "        return [self.bytes2id[b] for b in seq]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        bseq = b\"\".join(self.id2bytes[i] for i in ids)\n",
    "        return bseq.decode(\"utf-8\", errors=\"strict\")\n",
    "\n",
    "def main():\n",
    "    # 1. 读取原始文本\n",
    "    input_path = \"manual.txt\"\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        original = f.read()\n",
    "\n",
    "    # 2. 训练 BPE tokenizer\n",
    "    tok = Tokenizer()\n",
    "    tok.train(original, vocab_size=1024)\n",
    "\n",
    "    # 3. 用训练好的模型进行 encode & decode\n",
    "    ids = tok.encode(original)\n",
    "    decoded = tok.decode(ids)\n",
    "\n",
    "    # 4. 检查一致性\n",
    "    if decoded == original:\n",
    "        print(\"✅ 解码后文本与原文完全一致！\")\n",
    "    else:\n",
    "        print(\"❌ 存在差异，请检查合并逻辑或文本编码。\")\n",
    "        # 如需查看差异摘要，可以借助 difflib：\n",
    "        import difflib\n",
    "        diff = difflib.unified_diff(\n",
    "            original.splitlines(True),\n",
    "            decoded.splitlines(True),\n",
    "            fromfile=\"original\",\n",
    "            tofile=\"decoded\",\n",
    "        )\n",
    "        print(\"\".join(diff))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "原始 bytes == 重构 bytes? True\n",
      "原始文本 == 解码文本? True\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.merges = {}\n",
    "        self.id2bytes = {}\n",
    "        self.bytes2id = {}\n",
    "\n",
    "    def get_stats(self, seq):\n",
    "        stats = {}\n",
    "        for i in range(len(seq) - 1):\n",
    "            pair = (seq[i], seq[i+1])\n",
    "            stats[pair] = stats.get(pair, 0) + 1\n",
    "        return stats\n",
    "\n",
    "    def merge_pair(self, pair, seq):\n",
    "        merged = pair[0] + pair[1]\n",
    "        out, i = [], 0\n",
    "        while i < len(seq):\n",
    "            if i < len(seq)-1 and seq[i]==pair[0] and seq[i+1]==pair[1]:\n",
    "                out.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "        return out\n",
    "\n",
    "    def train(self, text, vocab_size):\n",
    "        data = list(text.encode(\"utf-8\"))\n",
    "        seq = [bytes([b]) for b in data]\n",
    "        # init vocab 0-255\n",
    "        for b in range(256):\n",
    "            bt = bytes([b])\n",
    "            self.id2bytes[b] = bt\n",
    "            self.bytes2id[bt] = b\n",
    "        next_id = 256\n",
    "\n",
    "        while len(self.id2bytes) < vocab_size:\n",
    "            stats = self.get_stats(seq)\n",
    "            if not stats:\n",
    "                break\n",
    "            best = max(stats, key=stats.get)\n",
    "            self.merges[best] = next_id\n",
    "            merged = best[0] + best[1]\n",
    "            self.id2bytes[next_id] = merged\n",
    "            self.bytes2id[merged] = next_id\n",
    "            next_id += 1\n",
    "            seq = self.merge_pair(best, seq)\n",
    "\n",
    "    def encode(self, text):\n",
    "        seq = [bytes([b]) for b in text.encode(\"utf-8\")]\n",
    "        for pair, idx in sorted(self.merges.items(), key=lambda x: x[1]):\n",
    "            i = 0\n",
    "            while i < len(seq)-1:\n",
    "                if seq[i]==pair[0] and seq[i+1]==pair[1]:\n",
    "                    seq[i:i+2] = [self.id2bytes[idx]]\n",
    "                else:\n",
    "                    i += 1\n",
    "        return [ self.bytes2id[b] for b in seq ]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        bseq = b\"\".join(self.id2bytes[i] for i in ids)\n",
    "        return bseq.decode(\"utf-8\", errors=\"strict\")\n",
    "\n",
    "    def decode_bytes(self, ids):\n",
    "        return b\"\".join(self.id2bytes[i] for i in ids)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. 原始 raw bytes\n",
    "    with open(\"manual.txt\", \"rb\") as f:\n",
    "        raw = f.read()\n",
    "    text = raw.decode(\"utf-8\")  # 不做任何 newline 转换\n",
    "\n",
    "    # 2. 训练、encode/decode\n",
    "    tok = Tokenizer()\n",
    "    tok.train(text, vocab_size=1024)\n",
    "    ids = tok.encode(text)\n",
    "    recon_raw = tok.decode_bytes(ids)\n",
    "    recon_text = tok.decode(ids)\n",
    "\n",
    "    # 3. 检查\n",
    "    print(\"原始 bytes == 重构 bytes?\", raw == recon_raw)\n",
    "    print(\"原始文本 == 解码文本?\", raw.decode(\"utf-8\") == recon_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        # 保存 BPE 合并规则：{ (b1, b2): new_id, ... }\n",
    "        self.merges = {}\n",
    "        # id -> bytes token\n",
    "        self.id2bytes = {}\n",
    "        # bytes token -> id\n",
    "        self.bytes2id = {}\n",
    "\n",
    "    def _get_stats(self, seq):\n",
    "        \"\"\"\n",
    "        统计字节序列中所有相邻对的出现次数。\n",
    "        seq: List[bytes]\n",
    "        返回: Dict[(bytes, bytes), int]\n",
    "        \"\"\"\n",
    "        stats = {}\n",
    "        for i in range(len(seq) - 1):\n",
    "            pair = (seq[i], seq[i+1])\n",
    "            stats[pair] = stats.get(pair, 0) + 1\n",
    "        return stats\n",
    "\n",
    "    def _merge_pair(self, pair, seq):\n",
    "        \"\"\"\n",
    "        在 seq 中将所有相邻的 pair=('x','y') 合并成 merged= x+y。\n",
    "        seq: List[bytes]\n",
    "        返回新的 List[bytes]\n",
    "        \"\"\"\n",
    "        merged = pair[0] + pair[1]\n",
    "        out = []\n",
    "        i = 0\n",
    "        while i < len(seq):\n",
    "            if i < len(seq) - 1 and seq[i] == pair[0] and seq[i+1] == pair[1]:\n",
    "                out.append(merged)\n",
    "                i += 2\n",
    "            else:\n",
    "                out.append(seq[i])\n",
    "                i += 1\n",
    "        return out\n",
    "\n",
    "    def train(self, text, vocab_size):\n",
    "        \"\"\"\n",
    "        训练 BPE：将整个 text 作为一个 byte-stream，初始 vocab 为 0–255 字节，\n",
    "        依次合并最频繁的 byte-pair，直到 vocab_size 为止。\n",
    "        \"\"\"\n",
    "        # 将 text 转为 UTF-8 byte 列表\n",
    "        data = list(text.encode(\"utf-8\"))\n",
    "        # 初始 seq：List[bytes([b])] \n",
    "        seq = [bytes([b]) for b in data]\n",
    "\n",
    "        # 初始化单字节 vocab\n",
    "        for b in range(256):\n",
    "            bt = bytes([b])\n",
    "            self.id2bytes[b] = bt\n",
    "            self.bytes2id[bt] = b\n",
    "        next_id = 256\n",
    "\n",
    "        # 迭代合并最频繁的对\n",
    "        while len(self.id2bytes) < vocab_size:\n",
    "            stats = self._get_stats(seq)\n",
    "            if not stats:\n",
    "                break\n",
    "            # 选出现次数最多的 pair\n",
    "            best = max(stats, key=stats.get)\n",
    "            merged = best[0] + best[1]\n",
    "            # 记录合并规则并分配新 id\n",
    "            self.merges[best] = next_id\n",
    "            self.id2bytes[next_id] = merged\n",
    "            self.bytes2id[merged] = next_id\n",
    "            next_id += 1\n",
    "            # 更新 seq\n",
    "            seq = self._merge_pair(best, seq)\n",
    "\n",
    "    def encode(self, text):\n",
    "        \"\"\"\n",
    "        Encode 输入字符串为 token ID 列表。\n",
    "        步骤：text -> UTF-8 bytes -> 按训练好的 merges 顺序合并 -> 映射为 ID\n",
    "        \"\"\"\n",
    "        seq = [bytes([b]) for b in text.encode(\"utf-8\")]\n",
    "        # 按 merges 的 id 升序（即训练时的顺序）依次做合并\n",
    "        for pair, idx in sorted(self.merges.items(), key=lambda x: x[1]):\n",
    "            i = 0\n",
    "            while i < len(seq) - 1:\n",
    "                if seq[i] == pair[0] and seq[i+1] == pair[1]:\n",
    "                    seq[i:i+2] = [self.id2bytes[idx]]\n",
    "                else:\n",
    "                    i += 1\n",
    "        # 最后把每个 bytes token 映射为 ID\n",
    "        return [self.bytes2id[b] for b in seq]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        \"\"\"\n",
    "        Decode ID 列表回原始字符串。\n",
    "        步骤：ID 列表 -> bytes token 列表拼接 -> UTF-8 decode\n",
    "        \"\"\"\n",
    "        bseq = b\"\".join(self.id2bytes[i] for i in ids)\n",
    "        return bseq.decode(\"utf-8\", errors=\"strict\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "一致吗？ True\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # 1. 读取原文（注意用二进制或禁用 newline 转换）\n",
    "    with open(\"manual.txt\", \"rb\") as f:\n",
    "        raw = f.read()\n",
    "    text = raw.decode(\"utf-8\")\n",
    "\n",
    "    # 2. 训练\n",
    "    tok = Tokenizer()\n",
    "    tok.train(text, vocab_size=1024)\n",
    "\n",
    "    # 3. 编码 & 解码\n",
    "    ids = tok.encode(text)\n",
    "    out = tok.decode(ids)\n",
    "\n",
    "    # 4. 检查\n",
    "    print(\"一致吗？\", out == text)  # 应该输出 True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
